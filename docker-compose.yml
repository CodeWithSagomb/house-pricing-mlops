services:
  # 1. Base de données (Stockage des métriques & logs & MLflow Backend)
  postgres:
    image: postgres:15-alpine
    container_name: mlops_postgres
    restart: always
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - mlops_net
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U postgres" ]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 10s

  # 2. Object Storage (Simulation AWS S3)
  minio:
    image: minio/minio:latest
    container_name: mlops_minio
    restart: always
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      MINIO_PROMETHEUS_AUTH_TYPE: public
    ports:
      - "9000:9000" # API S3
      - "9001:9001" # Console UI Web
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    networks:
      - mlops_net
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9000/minio/health/live" ]
      interval: 30s
      timeout: 20s
      retries: 3

  # 3. Setup Automatique (Crée les buckets pour nous)
  init-minio:
    image: minio/mc:latest
    container_name: mlops_init_minio
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c " # Attente de sécurité sleep 5; # Connexion au serveur MinIO mc alias set myminio http://minio:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD}; # Création des buckets si inexistants mc mb myminio/dvc-storage || true; mc mb myminio/mlflow-artifacts || true; # Rendre le bucket MLflow public (lecture seule) pour faciliter l'accès mc anonymous set download myminio/mlflow-artifacts; echo '✅ Buckets MinIO initialisés avec succès !'; exit 0; "
    networks:
      - mlops_net

  # 4. MLflow Tracking Server
  mlflow:
    build:
      context: ./docker/mlflow
      dockerfile: Dockerfile
    image: mlops-mlflow:local
    container_name: mlops_mlflow
    restart: always
    environment:
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
    command: >
      mlflow server --backend-store-uri postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB} --default-artifact-root s3://mlflow-artifacts --host 0.0.0.0 --port 5000 --allowed-hosts mlflow,mlflow:5000,localhost,localhost:5000,127.0.0.1,127.0.0.1:5000,mlops_mlflow,mlops_mlflow:5000
    ports:
      - "5000:5000"
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
    networks:
      - mlops_net
    healthcheck:
      test: [ "CMD-SHELL", "curl -sf http://localhost:5000/health || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 10s

  # 5. House Pricing API
  api:
    image: ghcr.io/codewithsagomb/house-pricing-mlops:latest
    build:
      context: .
      dockerfile: src/house_pricing/api/Dockerfile
    container_name: mlops_api
    restart: always
    environment:
      - PORT=8000
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
      - API_KEY=${API_KEY:-dev-secret-key}
      # Vars pour la DB (si besoin écriture logs)
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_HOST=postgres
    ports:
      - "8000:8000"
    depends_on:
      - mlflow
    volumes:
      - ./data/raw:/app/data/raw:ro # Raw data for drift detection reference
      - ./data/processed:/app/data/processed:ro # Processed data
      - api_logs:/app/logs # Persist logs and drift reports
    networks:
      - mlops_net
    healthcheck:
      test: [ "CMD-SHELL", "python -c \"import http.client; conn = http.client.HTTPConnection('localhost', 8000); conn.request('GET', '/health'); response = conn.getresponse(); exit(0) if response.status == 200 else exit(1)\"" ]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 45s

  # 6. Frontend Dashboard
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        NEXT_PUBLIC_API_URL: http://localhost:8000
    image: mlops-frontend:local
    container_name: mlops_frontend
    restart: unless-stopped
    ports:
      - "3001:3000"
    environment:
      - NODE_ENV=production
    networks:
      - mlops_net
    depends_on:
      api:
        condition: service_healthy
    healthcheck:
      test: [ "CMD", "wget", "--spider", "-q", "http://localhost:3000/" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s

  # 7. Prometheus - Metrics Collection
  prometheus:
    image: prom/prometheus:v2.47.0
    container_name: mlops_prometheus
    restart: always
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/prometheus/rules:/etc/prometheus/rules:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'
      - '--storage.tsdb.retention.time=30d'
    networks:
      - mlops_net
    depends_on:
      - api

  # 8. Node Exporter - System Metrics (CPU, Memory, Disk, Network)
  node-exporter:
    image: prom/node-exporter:v1.7.0
    container_name: mlops_node_exporter
    restart: always
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--path.rootfs=/rootfs'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    networks:
      - mlops_net

  # 9. cAdvisor - Container Metrics
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.47.0
    container_name: mlops_cadvisor
    restart: always
    ports:
      - "8080:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    privileged: true
    networks:
      - mlops_net

  # 10. PostgreSQL Exporter - Database Metrics
  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:v0.15.0
    container_name: mlops_postgres_exporter
    restart: always
    ports:
      - "9187:9187"
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}?sslmode=disable"
    networks:
      - mlops_net
    depends_on:
      postgres:
        condition: service_healthy

  # 7. Grafana - Dashboards & Visualization
  grafana:
    image: grafana/grafana:10.0.0
    container_name: mlops_grafana
    restart: always
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - grafana_data:/var/lib/grafana
    networks:
      - mlops_net
    depends_on:
      - prometheus

  # ============================================
  # AIRFLOW ORCHESTRATION (LocalExecutor)
  # ============================================

  # Airflow Database Initialization
  airflow-init:
    build:
      context: .
      dockerfile: airflow/Dockerfile
    container_name: mlops_airflow_init
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/airflow_db
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY}
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW_SECRET_KEY}
      - _AIRFLOW_DB_MIGRATE=true
      - _AIRFLOW_WWW_USER_CREATE=true
      - _AIRFLOW_WWW_USER_USERNAME=${AIRFLOW_ADMIN_USER}
      - _AIRFLOW_WWW_USER_PASSWORD=${AIRFLOW_ADMIN_PASSWORD}
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db migrate
        airflow users create --username ${AIRFLOW_ADMIN_USER:-admin} --password ${AIRFLOW_ADMIN_PASSWORD:-admin} --firstname Admin --lastname User --role Admin --email ${AIRFLOW_ADMIN_EMAIL:-admin@mlops.local} || true
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - mlops_net
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
      - ./config:/opt/airflow/config
      - ./.dvc:/opt/airflow/.dvc:ro

  # Airflow Webserver (UI)
  airflow-webserver:
    build:
      context: .
      dockerfile: airflow/Dockerfile
    container_name: mlops_airflow_webserver
    restart: always
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/airflow_db
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY}
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW_SECRET_KEY}
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
    ports:
      - "8081:8080"
    command: webserver
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    networks:
      - mlops_net
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
      - ./config:/opt/airflow/config
      - ./.dvc:/opt/airflow/.dvc:ro
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080/health" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # Airflow Scheduler
  airflow-scheduler:
    build:
      context: .
      dockerfile: airflow/Dockerfile
    container_name: mlops_airflow_scheduler
    restart: always
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/airflow_db
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY}
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW_SECRET_KEY}
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
      - API_HOST=http://api:8000
    command: scheduler
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    networks:
      - mlops_net
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
      - ./config:/opt/airflow/config
      - ./.dvc:/opt/airflow/.dvc:ro

# Persistance des données (Volumes Docker)
volumes:
  postgres_data:
  minio_data:
  prometheus_data:
  grafana_data:
  api_logs:
    # Drift reports and API logs

    # Réseau interne isolé
networks:
  mlops_net:
    driver: bridge
