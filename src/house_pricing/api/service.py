import logging
from functools import lru_cache

import joblib
import mlflow.sklearn
import pandas as pd

from house_pricing.api.config import get_settings
from house_pricing.api.exceptions import ModelNotLoadedError, PredictionError

logger = logging.getLogger("api.service")


class ModelService:
    def __init__(self):
        self.model = None
        self.preprocessor = None
        self.model_version = "unknown"

    def load_artifacts(self):
        """Charge le mod√®le et le pr√©processeur."""
        logger.info("üîå Chargement des artefacts ML...")

        # 1. Setup MLflow
        settings = get_settings()
        mlflow.set_tracking_uri(settings.MLFLOW_TRACKING_URI)
        client = mlflow.MlflowClient()

        # 2. R√©solution de l'alias (ex: "champion") -> Version r√©elle (ex: "v2")
        mv = client.get_model_version_by_alias(
            settings.MODEL_NAME, settings.MODEL_ALIAS
        )
        self.model_version = str(mv.version)
        run_id = mv.run_id

        logger.info(
            f"üîç Mod√®le identifi√© : {settings.MODEL_NAME} version {self.model_version} (Run ID: {run_id})"
        )

        # 3. T√©l√©chargement & Chargement du Preprocessor (Dynamique)
        try:
            # On t√©l√©charge l'artifact "preprocessor/preprocessor.pkl" depuis le run associ√© au mod√®le
            local_path = mlflow.artifacts.download_artifacts(
                run_id=run_id,
                artifact_path="preprocessor/preprocessor.pkl",
                dst_path="/tmp",  # On t√©l√©charge dans /tmp
            )
            self.preprocessor = joblib.load(local_path)
            logger.info("‚úÖ Preprocessor t√©l√©charg√© et charg√© depuis MLflow.")
        except Exception as e:
            logger.error(
                f"‚ùå Impossible de charger le preprocessor depuis MLflow : {e}"
            )
            # Fallback local (optionnel, pour dev)
            logger.warning("‚ö†Ô∏è Tentative de fallback local...")
            self.preprocessor = joblib.load(settings.PREPROCESSOR_PATH)

        # 4. Chargement du Mod√®le
        model_uri = f"models:/{settings.MODEL_NAME}@{settings.MODEL_ALIAS}"
        self.model = mlflow.sklearn.load_model(model_uri)

        logger.info(f"‚úÖ Mod√®le v{self.model_version} charg√© avec succ√®s.")

    def predict(self, features: dict) -> tuple[float, str]:
        """Effectue la pr√©diction."""
        if not self.model or not self.preprocessor:
            raise ModelNotLoadedError("Le mod√®le n'est pas charg√©.")

        try:
            # Conversion dict -> DataFrame
            df = pd.DataFrame([features])

            # Transform & Predict
            X_processed = self.preprocessor.transform(df)
            prediction = self.model.predict(X_processed)

            return float(prediction[0]), str(self.model_version)
        except Exception as e:
            logger.error(f"Erreur pr√©diction: {e}")
            raise PredictionError(f"Erreur interne du mod√®le: {e}")


@lru_cache
def get_model_service():
    """Fournit l'instance unique (Singleton) du service ML."""
    return ModelService()
